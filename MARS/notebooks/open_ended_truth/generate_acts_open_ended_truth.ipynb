{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Truth_is_Universal/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import configparser\n",
    "import glob\n",
    "\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read('config.ini')\n",
    "\n",
    "# class Hook:\n",
    "#     def __init__(self):\n",
    "#         self.out = None\n",
    "\n",
    "#     def __call__(self, module, module_inputs, module_outputs):\n",
    "#         self.out, _ = module_outputs\n",
    "\n",
    "# def load_model(model_family: str, model_size: str, model_type: str, device: str):\n",
    "#     model_path = os.path.join(config[model_family]['weights_directory'], \n",
    "#                               config[model_family][f'{model_size}_{model_type}_subdir'])\n",
    "    \n",
    "#     try:\n",
    "#         if model_family == 'Llama2':\n",
    "#             tokenizer = LlamaTokenizer.from_pretrained(str(model_path))\n",
    "#             model = LlamaForCausalLM.from_pretrained(str(model_path))\n",
    "#             tokenizer.bos_token = '<s>'\n",
    "#         else:\n",
    "#             tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
    "#             model = AutoModelForCausalLM.from_pretrained(str(model_path))\n",
    "#         if model_family == \"Gemma2\": # Gemma2 requires bfloat16 precision which is only available on new GPUs\n",
    "#             model = model.to(t.bfloat16) # Convert the model to bfloat16 precision\n",
    "#         else:\n",
    "#             model = model.half()  # storing model in float32 precision -> conversion to float16\n",
    "#         return tokenizer, model.to(device)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading model: {e}\")\n",
    "#         raise\n",
    "\n",
    "# def load_statements(dataset_name):\n",
    "#     \"\"\"\n",
    "#     Load statements from csv file, return list of strings.\n",
    "#     \"\"\"\n",
    "#     dataset = pd.read_csv(f\"datasets/open_ended_truth/{dataset_name}.csv\")\n",
    "#     statements = dataset['question_with_answer'].tolist()\n",
    "#     return statements\n",
    "\n",
    "# def get_acts(statements, tokenizer, model, layers, device):\n",
    "#     \"\"\"\n",
    "#     Get given layer activations for the statements. \n",
    "#     Return dictionary of stacked activations.\n",
    "#     \"\"\"\n",
    "#     # attach hooks\n",
    "#     hooks, handles = [], []\n",
    "#     for layer in layers:\n",
    "#         hook = Hook()\n",
    "#         handle = model.model.layers[layer].register_forward_hook(hook)\n",
    "#         hooks.append(hook), handles.append(handle)\n",
    "    \n",
    "#     # get activations\n",
    "#     acts = {layer : [] for layer in layers}\n",
    "#     for statement in tqdm(statements):\n",
    "#         input_ids = tokenizer.encode(statement, return_tensors=\"pt\").to(device)\n",
    "#         model(input_ids)\n",
    "#         for layer, hook in zip(layers, hooks):\n",
    "#             acts[layer].append(hook.out[0, -1])\n",
    "    \n",
    "#     for layer, act in acts.items():\n",
    "#         acts[layer] = t.stack(act).float()\n",
    "    \n",
    "#     # remove hooks\n",
    "#     for handle in handles:\n",
    "#         handle.remove()\n",
    "    \n",
    "#     return acts\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     \"\"\"\n",
    "#     read statements from dataset, record activations in given layers, and save to specified files\n",
    "#     \"\"\"\n",
    "#     parser = argparse.ArgumentParser(description=\"Generate activations for statements in a dataset\")\n",
    "#     parser.add_argument(\"--model_family\", default=\"Llama3\", help=\"Model family to use. Options are Llama2, Llama3, Gemma, Gemma2 or Mistral.\")\n",
    "#     parser.add_argument(\"--model_size\", default=\"8B\",\n",
    "#                         help=\"Size of the model to use. Options for Llama3 are 8B or 70B\")\n",
    "#     parser.add_argument(\"--model_type\", default=\"base\", help=\"Whether to choose base or chat model. Options are base or chat.\")\n",
    "#     parser.add_argument(\"--layers\", nargs='+', \n",
    "#                         help=\"Layers to save embeddings from.\")\n",
    "#     parser.add_argument(\"--datasets\", nargs='+',\n",
    "#                         help=\"Names of datasets, without .csv extension\")\n",
    "#     parser.add_argument(\"--output_dir\", default=\"acts\",\n",
    "#                         help=\"Directory to save activations to.\")\n",
    "#     parser.add_argument(\"--device\", default=\"cpu\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # datasets = args.datasets\n",
    "    # if datasets == ['all_topic_specific']:\n",
    "    #     datasets = ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts',\n",
    "    #                 'neg_cities', 'neg_sp_en_trans', 'neg_inventors', 'neg_animal_class', 'neg_element_symb', 'neg_facts',\n",
    "    #                 'cities_conj', 'sp_en_trans_conj', 'inventors_conj', 'animal_class_conj', 'element_symb_conj', 'facts_conj',\n",
    "    #                 'cities_disj', 'sp_en_trans_disj', 'inventors_disj', 'animal_class_disj', 'element_symb_disj', 'facts_disj',\n",
    "    #                 'larger_than', 'smaller_than', \"cities_de\", \"neg_cities_de\", \"sp_en_trans_de\", \"neg_sp_en_trans_de\", \"inventors_de\", \"neg_inventors_de\", \"animal_class_de\",\n",
    "    #               \"neg_animal_class_de\", \"element_symb_de\", \"neg_element_symb_de\", \"facts_de\", \"neg_facts_de\"]\n",
    "    # if datasets == ['all']:\n",
    "    #     datasets = []\n",
    "    #     for file_path in glob.glob('datasets/**/*.csv', recursive=True):\n",
    "    #         dataset_name = os.path.relpath(file_path, 'datasets').replace('.csv', '')\n",
    "    #         datasets.append(dataset_name)\n",
    "\n",
    "    # t.set_grad_enabled(False)\n",
    "    # tokenizer, model = load_model(args.model_family, args.model_size, args.model_type, args.device)\n",
    "    \n",
    "    # for dataset in datasets:\n",
    "    #     statements = load_statements(dataset)\n",
    "    #     layers = [int(layer) for layer in args.layers]\n",
    "    #     if layers == [-1]:\n",
    "    #         layers = list(range(len(model.model.layers)))\n",
    "    #     save_dir = f\"{args.output_dir}/{args.model_family}/{args.model_size}/{args.model_type}/{dataset}/\"\n",
    "    #     if not os.path.exists(save_dir):\n",
    "    #         os.makedirs(save_dir)\n",
    "\n",
    "    #     for idx in range(0, len(statements), 25):\n",
    "    #         acts = get_acts(statements[idx:idx + 25], tokenizer, model, layers, args.device)\n",
    "    #         for layer, act in acts.items():\n",
    "    #                 t.save(act, f\"{save_dir}/layer_{layer}_{idx}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, module, module_inputs, module_outputs):\n",
    "        self.out, _ = module_outputs\n",
    "\n",
    "def load_model(model_family: str, model_size: str, model_type: str, device: str):\n",
    "    # model_path = os.path.join(config[model_family]['weights_directory'], \n",
    "    #                           config[model_family][f'{model_size}_{model_type}_subdir'])\n",
    "    \n",
    "    model_path = \"../../../../models/llama3_8b_chat_hf\"\n",
    "    \n",
    "    try:\n",
    "        if model_family == 'Llama2':\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(str(model_path))\n",
    "            model = LlamaForCausalLM.from_pretrained(str(model_path))\n",
    "            tokenizer.bos_token = '<s>'\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
    "            model = AutoModelForCausalLM.from_pretrained(str(model_path))\n",
    "        if model_family == \"Gemma2\": # Gemma2 requires bfloat16 precision which is only available on new GPUs\n",
    "            model = model.to(t.bfloat16) # Convert the model to bfloat16 precision\n",
    "        else:\n",
    "            model = model.half()  # storing model in float32 precision -> conversion to float16\n",
    "        return tokenizer, model.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_statements(dataset_name):\n",
    "    \"\"\"\n",
    "    Load statements from csv file, return list of strings.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(f\"../../../datasets/open_ended_truth/{dataset_name}.csv\")\n",
    "    statements = dataset['question_with_answer'].tolist()\n",
    "    return statements\n",
    "\n",
    "def get_acts(statements, tokenizer, model, layers, device):\n",
    "    \"\"\"\n",
    "    Get given layer activations for the statements. \n",
    "    Return dictionary of stacked activations.\n",
    "    \"\"\"\n",
    "    # attach hooks\n",
    "    hooks, handles = [], []\n",
    "    for layer in layers:\n",
    "        hook = Hook()\n",
    "        handle = model.model.layers[layer].register_forward_hook(hook)\n",
    "        hooks.append(hook), handles.append(handle)\n",
    "    \n",
    "    # get activations\n",
    "    acts = {layer : [] for layer in layers}\n",
    "    for statement in tqdm(statements):\n",
    "        input_ids = tokenizer.encode(statement, return_tensors=\"pt\").to(device)\n",
    "        model(input_ids)\n",
    "        for layer, hook in zip(layers, hooks):\n",
    "            acts[layer].append(hook.out[0, -1])\n",
    "    \n",
    "    for layer, act in acts.items():\n",
    "        acts[layer] = t.stack(act).float()\n",
    "    \n",
    "    # remove hooks\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"cities_questions\",\n",
    "    \"element_symb_questions\",\n",
    "    \"inventors_questions\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_family = \"Llama3\"\n",
    "model_size = \"8B\"\n",
    "model_type = \"chat\"\n",
    "device = \"cuda:0\"\n",
    "layers = [12]\n",
    "output_dir = \"../../../acts/open_ended_truth_acts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.10s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.64it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.75it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.17it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.75it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.43it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.27it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.85it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.84it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.03it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.02it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.28it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.59it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.36it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.16it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.15it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.87it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.23it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.95it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.52it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.93it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.42it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.57it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 40.07it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.05it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.34it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.81it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.68it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.34it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.90it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.70it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.96it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.28it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.88it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.83it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.73it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.53it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.60it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.73it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.61it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.97it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.01it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.53it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.01it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.86it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.71it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.88it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.89it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.42it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.24it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.73it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.79it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.61it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.77it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.82it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 38.54it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.74it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.77it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.84it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 39.39it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.51it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 40.15it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 40.25it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 40.23it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.58it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 40.19it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.45it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 40.97it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.46it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.92it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.84it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.10it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.86it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.78it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.80it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 40.11it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.98it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 40.14it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.90it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.89it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.38it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.98it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 39.98it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 40.10it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 42.50it/s]\n"
     ]
    }
   ],
   "source": [
    "t.set_grad_enabled(False)\n",
    "tokenizer, model = load_model(model_family, model_size, model_type, device)\n",
    "\n",
    "for dataset in datasets:\n",
    "    statements = load_statements(dataset)\n",
    "    layers = [int(layer) for layer in layers]\n",
    "    if layers == [-1]:\n",
    "        layers = list(range(len(model.model.layers)))\n",
    "    save_dir = f\"{output_dir}/{model_family}/{model_size}/{model_type}/{dataset}/\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    for idx in range(0, len(statements), 25):\n",
    "        acts = get_acts(statements[idx:idx + 25], tokenizer, model, layers, device)\n",
    "        for layer, act in acts.items():\n",
    "                t.save(act, f\"{save_dir}/layer_{layer}_{idx}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
